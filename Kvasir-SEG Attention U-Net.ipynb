{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7681479,"sourceType":"datasetVersion","datasetId":1316959},{"sourceId":516991,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":407804,"modelId":425675},{"sourceId":518869,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":408642,"modelId":426501},{"sourceId":519384,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":408954,"modelId":426810},{"sourceId":521138,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":409852,"modelId":427700}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data\n\nhttps://www.kaggle.com/datasets/debeshjha1/kvasirseg","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport csv\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\nfrom skimage.segmentation import find_boundaries\nfrom scipy.ndimage import distance_transform_edt as edt\nimport scipy.ndimage as ndi\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    #Set seed for reproducibility across all libraries\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # Make PyTorch deterministic (slower but more reproducible)\n    torch.use_deterministic_algorithms(True)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Set environment variable for additional reproducibility\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Train and Val File Names","metadata":{}},{"cell_type":"code","source":"train_files = \"/kaggle/input/kvasirseg/train.txt\"\nval_files = \"/kaggle/input/kvasirseg/val.txt\"\n# train_files = \"train.txt\"\n# val_files = \"val.txt\"\n\ntrain_df = pd.read_csv(train_files, header=None, names=['file_name'])\nval_df = pd.read_csv(val_files, header=None, names=['file_name'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dir = \"/kaggle/input/kvasirseg/Kvasir-SEG/Kvasir-SEG/images\"\nmask_dir = \"/kaggle/input/kvasirseg/Kvasir-SEG/Kvasir-SEG/masks\"\nimage = Image.open(os.path.join(image_dir, train_df['file_name'][0]+\".jpg\"))\nmask = Image.open(os.path.join(mask_dir, train_df['file_name'][0]+\".jpg\")).convert(\"L\")\n\n# conver to numpy\nimage = np.array(image)\nmask = np.array(mask) / 255.0\nmask = (mask > 0.5).astype(np.float32)\n\n# show image\nplt.imshow(image)\nplt.show()\nplt.imshow(mask)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UNet Architecture","metadata":{}},{"cell_type":"markdown","source":"## UNet Parts","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv_op = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.conv_op(x)\n\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int, stride_x=2):\n        super().__init__()\n        # Gating signal path (from decoder)\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        # Skip connection path (from encoder)\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=stride_x, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        # Attention map\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        psi_upsampled = F.interpolate(psi, size=x.size()[2:], mode='bilinear', align_corners=True)\n        return x * psi_upsampled\n\n\nclass DownSample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = DoubleConv(in_channels, out_channels)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        down = self.conv(x)\n        p = self.pool(down)\n\n        return down, p\n\n\nclass UpSample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x1, x2], 1)\n        return self.conv(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## UNet Class","metadata":{}},{"cell_type":"code","source":"class AttentionUNet(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.down_convolution_1 = DownSample(in_channels, 64)\n        self.down_convolution_2 = DownSample(64, 128)\n        self.down_convolution_3 = DownSample(128, 256)\n        self.down_convolution_4 = DownSample(256, 512)\n\n        self.bottle_neck = DoubleConv(512, 1024)\n\n        self.att4 = AttentionGate(F_g=1024, F_l=512, F_int=512, stride_x=2)\n        self.att3 = AttentionGate(F_g=512, F_l=256, F_int=256, stride_x=2)\n        self.att2 = AttentionGate(F_g=256, F_l=128, F_int=128, stride_x=2)\n        self.att1 = AttentionGate(F_g=128, F_l=64, F_int=64, stride_x=2)\n\n        self.up_convolution_1 = UpSample(1024, 512)\n        self.up_convolution_2 = UpSample(512, 256)\n        self.up_convolution_3 = UpSample(256, 128)\n        self.up_convolution_4 = UpSample(128, 64)\n\n        self.out = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n\n    def forward(self, x):\n        down_1, p1 = self.down_convolution_1(x)\n        down_2, p2 = self.down_convolution_2(p1)\n        down_3, p3 = self.down_convolution_3(p2)\n        down_4, p4 = self.down_convolution_4(p3)\n\n        b = self.bottle_neck(p4)\n\n        # Decoder with Attention\n        att4 = self.att4(g=b, x=down_4)\n        x4 = self.up_convolution_1(b, att4)\n\n        att3 = self.att3(g=x4, x=down_3)\n        x3 = self.up_convolution_2(x4, att3)\n\n        att2 = self.att2(g=x3, x=down_2)\n        x2 = self.up_convolution_3(x3, att2)\n\n        att1 = self.att1(g=x2, x=down_1)\n        x1 = self.up_convolution_4(x2, att1)\n\n        # Final output\n        out = self.out(x1)\n        return out\n\n        # up_1 = self.up_convolution_1(b, down_4)\n        # up_2 = self.up_convolution_2(up_1, down_3)\n        # up_3 = self.up_convolution_3(up_2, down_2)\n        # up_4 = self.up_convolution_4(up_3, down_1)\n\n        # out = self.out(up_4)\n        # return out\n\n\nif __name__ == \"__main__\":\n    double_conv = DoubleConv(256, 256)\n    # print(double_conv)\n\n    input_image = torch.rand((1, 3, 512, 512))\n    model = AttentionUNet(3, 10)\n    # print(model)\n    output = model(input_image)\n    print(output.size()) # Expected size: [1, 10, 512, 512]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Class\n","metadata":{}},{"cell_type":"code","source":"class KvasirDataset(Dataset):\n    def __init__(self, root_path, filename_df, target_size=(320, 320), boundary_dist_max=50):\n        self.root = root_path\n        self.df = filename_df\n        self.images = sorted([os.path.join(root_path, \"images\", f) + \".jpg\" for f in self.df['file_name']])\n        self.masks = sorted([os.path.join(root_path, \"masks\", f) + \".jpg\" for f in self.df['file_name']])\n\n        self.transform = A.Compose([\n            A.Resize(height=320, width=320),\n            # A.HorizontalFlip(p=0.5),\n            # A.VerticalFlip(p=0.5),\n            # A.Rotate(limit=30, p=0.5),\n            # A.RandomBrightnessContrast(p=0.5),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2()\n        ])\n        self.boundary_dist_max = boundary_dist_max\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.images[idx]).convert(\"RGB\")\n        m = Image.open(self.masks[idx]).convert(\"L\")\n\n        augmented = self.transform(image=np.array(img), mask=np.array(m))\n        img_t = augmented['image']\n        mask_t = augmented['mask'].unsqueeze(0).float()\n\n        # Convert grayscale mask (0..1) into binary 0 or 1\n        mask_bin = (mask_t > 0.5).float()\n\n        dist_map = self._compute_signed_distance(mask_bin.numpy()[0, :, :])\n        dist_map_t = torch.from_numpy(dist_map).unsqueeze(0).float()\n\n        # return img_t, mask_bin, self.df['file_name'].iloc[idx]\n        return img_t, mask_bin, dist_map_t, self.df['file_name'].iloc[idx]\n\n    def _compute_signed_distance(self, bin_mask):\n        posmask = bin_mask.astype(bool)\n        negmask = ~posmask\n        dist_out = edt(negmask)\n        dist_in = edt(posmask)\n        signed = dist_out - dist_in\n        signed = np.clip(signed, -self.boundary_dist_max, self.boundary_dist_max)\n        return signed / self.boundary_dist_max\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"class StandardDiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super(StandardDiceLoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        pred = pred.contiguous().view(-1)\n        target = target.contiguous().view(-1)\n        intersection = (pred * target).sum()\n        dice_score = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n        return 1 - dice_score\n        \nclass AdaptiveTVMFDiceLoss:\n    def __init__(self, num_classes=1, kappa_max=100.0, lambda_kappa=10.0):\n        self.kappa = torch.zeros(num_classes, requires_grad=False)\n        self.kappa_max = kappa_max\n        self.lambda_kappa = lambda_kappa\n\n    def update_kappa(self, dice_per_class):\n        self.kappa = torch.min(self.lambda_kappa * dice_per_class,\n                               torch.tensor(self.kappa_max))\n\n    def __call__(self, preds, targets):\n        B, C = preds.shape[:2]\n        preds_flat = preds.view(B, C, -1)\n        targets_flat = targets.view(B, C, -1)\n        dot = (preds_flat * targets_flat).sum(-1)\n        norm = torch.norm(preds_flat, dim=-1) * torch.norm(targets_flat, dim=-1) + 1e-6\n        cos_sim = dot / norm\n        k = self.kappa.view(1, C).to(preds.device)\n        t = (1 + cos_sim) / (1 + k * (1 - cos_sim)) - 1\n        return torch.mean((1 - t) ** 2)\n\n\nclass BoundaryLoss(nn.Module):\n    def forward(self, preds, dist_map):\n        return torch.mean(preds[:, 0, :, :] * dist_map)\n\nclass StandardDiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super(StandardDiceLoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        pred = pred.contiguous().view(-1)\n        target = target.contiguous().view(-1)\n        intersection = (pred * target).sum()\n        dice_score = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n        return 1 - dice_score\n        \nclass AdaptiveTVMFDiceLoss:\n    def __init__(self, num_classes=1, kappa_max=100.0, lambda_kappa=10.0):\n        self.kappa = torch.zeros(num_classes, requires_grad=False)\n        self.kappa_max = kappa_max\n        self.lambda_kappa = lambda_kappa\n\n    def update_kappa(self, dice_per_class):\n        self.kappa = torch.min(self.lambda_kappa * dice_per_class,\n                               torch.tensor(self.kappa_max))\n\n    def __call__(self, preds, targets):\n        B, C = preds.shape[:2]\n        preds_flat = preds.view(B, C, -1)\n        targets_flat = targets.view(B, C, -1)\n        dot = (preds_flat * targets_flat).sum(-1)\n        norm = torch.norm(preds_flat, dim=-1) * torch.norm(targets_flat, dim=-1) + 1e-6\n        cos_sim = dot / norm\n        k = self.kappa.view(1, C).to(preds.device)\n        t = (1 + cos_sim) / (1 + k * (1 - cos_sim)) - 1\n        return torch.mean((1 - t) ** 2)\n\n\nclass BoundaryLoss(nn.Module):\n    def forward(self, preds, dist_map):\n        return torch.mean(preds[:, 0, :, :] * dist_map)\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n\n\nclass TverskyLoss(nn.Module):\n    def __init__(self, alpha=0.3, beta=0.7, smooth=1e-6):\n        super(TverskyLoss, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.smooth = smooth\n\n    def forward(self, inputs, targets):\n        inputs = torch.sigmoid(inputs)\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        TP = (inputs * targets).sum()\n        FP = ((1 - targets) * inputs).sum()\n        FN = (targets * (1 - inputs)).sum()\n        Tversky = (TP + self.smooth) / (TP + self.alpha * FP + self.beta * FN + self.smooth)\n        return 1 - Tversky\n\n\nclass EdgeAwareLoss(nn.Module):\n    def __init__(self, edge_weight=1e-4):\n        super().__init__()\n        self.edge_weight = edge_weight\n        # Define sobel kernels as buffers without device yet\n        sobel_x = torch.tensor([[[[-1, 0, 1],\n                                  [-2, 0, 2],\n                                  [-1, 0, 1]]]], dtype=torch.float32)\n        sobel_y = torch.tensor([[[[-1, -2, -1],\n                                  [ 0,  0,  0],\n                                  [ 1,  2,  1]]]], dtype=torch.float32)\n        self.register_buffer('sobel_x', sobel_x)\n        self.register_buffer('sobel_y', sobel_y)\n\n    def forward(self, inputs, targets):\n        inputs = torch.sigmoid(inputs)\n        # Move kernels to input's device dynamically\n        sobel_x = self.sobel_x.to(inputs.device)\n        sobel_y = self.sobel_y.to(inputs.device)\n\n        pred_edges_x = F.conv2d(inputs, sobel_x, padding=1)\n        pred_edges_y = F.conv2d(inputs, sobel_y, padding=1)\n        pred_edges = torch.sqrt(pred_edges_x ** 2 + pred_edges_y ** 2 + 1e-6)\n\n        target_edges_x = F.conv2d(targets, sobel_x, padding=1)\n        target_edges_y = F.conv2d(targets, sobel_y, padding=1)\n        target_edges = torch.sqrt(target_edges_x ** 2 + target_edges_y ** 2 + 1e-6)\n\n        loss = F.l1_loss(pred_edges, target_edges)\n        return self.edge_weight * loss\n\nclass HausdorffDistanceLoss(nn.Module):\n    def __init__(self):\n        super(HausdorffDistanceLoss, self).__init__()\n\n    def forward(self, preds, targets):\n        preds = preds.squeeze(1)  # [B,H,W]\n        targets = targets.squeeze(1)\n\n        preds_np = preds.detach().cpu().numpy()\n        targets_np = targets.detach().cpu().numpy()\n\n        losses = []\n        for p, t in zip(preds_np, targets_np):\n            p_bin = (p > 0.5).astype(np.uint8)\n            t_bin = t.astype(np.uint8)\n\n            dt_pred = ndi.distance_transform_edt(1 - p_bin)\n            dt_gt = ndi.distance_transform_edt(1 - t_bin)\n\n            hd_pred = np.percentile(dt_gt[p_bin == 1], 95) if np.any(p_bin == 1) else 0\n            hd_gt = np.percentile(dt_pred[t_bin == 1], 95) if np.any(t_bin == 1) else 0\n\n            hd = max(hd_pred, hd_gt)\n            losses.append(hd)\n\n        return torch.tensor(losses, device=preds.device).mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"def compute_metrics_per_image(pred_mask, gt_mask):\n    pred = pred_mask.flatten()\n    gt = gt_mask.flatten()\n\n    dice = (2*np.sum(pred*gt) + 1e-6) / (np.sum(pred) + np.sum(gt) + 1e-6)\n    iou = jaccard_score(gt, pred)\n    accuracy = (np.sum(pred == gt) / len(gt))\n    precision = precision_score(gt, pred, zero_division=0)\n    recall = recall_score(gt, pred, zero_division=0)\n    f1 = f1_score(gt, pred, zero_division=0)\n\n    # Boundary metrics\n    bgt = find_boundaries(gt_mask, mode='inner').astype(int)\n    bpr = find_boundaries(pred_mask, mode='inner').astype(int)\n    tp = np.sum((bpr == 1) & (bgt == 1))\n    fp = np.sum((bpr == 1) & (bgt == 0))\n    fn = np.sum((bgt == 1) & (bpr == 0))\n    prec = tp / (tp + fp + 1e-6)\n    rec = tp / (tp + fn + 1e-6)\n    bf1 = 2 * prec * rec / (prec + rec + 1e-6)\n    inter = np.sum((bpr & bgt))\n    union = np.sum(((bpr + bgt) > 0))\n    biou = inter / (union + 1e-6)\n\n    # Per-class accuracy (binary class: bg and fg)\n    total_pixels = len(gt)\n    bg_acc = np.sum((pred == 0) & (gt == 0)) / (np.sum(gt == 0) + 1e-6)\n    fg_acc = np.sum((pred == 1) & (gt == 1)) / (np.sum(gt == 1) + 1e-6)\n\n    return dice, iou, bf1, biou, accuracy, precision, recall, f1, bg_acc, fg_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_branch(branch_name, config, train_df, val_df, out_dir, data_root,\n                 BATCH=10, EPOCHS=20, LR=3e-4, seed=42):\n\n    os.makedirs(out_dir, exist_ok=True)\n    m_csv = os.path.join(out_dir, \"metrics_per_image.csv\")\n    l_csv = os.path.join(out_dir, \"loss_log.csv\")\n\n    with open(m_csv, 'w', newline='') as f:\n        csv.writer(f).writerow(['image_id', 'epoch', 'branch', 'dice', 'iou', 'bf1', 'biou',\n                                'accuracy', 'precision', 'recall', 'f1', 'bg_acc', 'fg_acc'])\n    with open(l_csv, 'w', newline='') as f:\n        csv.writer(f).writerow(['epoch', 'train_loss', 'val_loss', 'ce', 'dice_term', 'bnd_term', 'accuracy', 'bg_acc', 'fg_acc', 'time_s'])\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Datasets & loaders\n    train_ds = KvasirDataset(data_root, train_df)\n    val_ds = KvasirDataset(data_root, val_df)\n\n    tr_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n                       generator=torch.Generator().manual_seed(seed))\n    vl_dl = DataLoader(val_ds, batch_size=1, shuffle=False,\n                       generator=torch.Generator().manual_seed(seed))\n\n    # Model & optimizer\n    model = AttentionUNet(in_channels=3, num_classes=1).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n\n    # Loss function init based on config\n    dice_loss_fn = StandardDiceLoss() if config.get('use_dice') else None\n    adaptive_dice_fn = AdaptiveTVMFDiceLoss(num_classes=1) if config.get('use_tvmf') else None\n    focal_loss_fn = FocalLoss() if config.get('use_focal') else None\n    tversky_loss_fn = TverskyLoss() if config.get('use_tversky') else None\n    bnd_loss_fn = BoundaryLoss() if config.get('use_boundary') else None\n\n    if config.get('use_boundary'):\n        btype = config.get('boundary_type', 'hausdorff')\n        if btype == 'weighted':\n            bnd_loss_fn = EdgeAwareLoss(edge_weight=config.get('w_bnd', 1e-4))\n        elif btype == 'hausdorff':\n            bnd_loss_fn = HausdorffDistanceLoss()\n\n    w_ce = config.get('w_ce', 1.0)\n    w_dice = config.get('w_dice', 0.0)\n    w_bnd = config.get('w_bnd', 0.0)\n    w_focal = config.get('w_focal', 0.0)\n    w_tversky = config.get('w_tversky', 0.0)\n    w_adaptive = config.get('w_adaptive', 0.0)\n    w_bnd = config.get('w_bnd', 0.0)\n\n    for epoch in range(1, EPOCHS + 1):\n        t0 = time.time()\n        model.train()\n        tr_ce = tr_d = tr_b = tr_f = tr_t = tr_a = tot = 0.0\n\n        for img, mask, dist, _ in tqdm(tr_dl, desc=f\"[{branch_name}] Train\"):\n            img, mask, dist = img.to(device), mask.to(device), dist.to(device)\n            logits = model(img)\n            prob = torch.sigmoid(logits)\n\n            ce_loss = F.binary_cross_entropy_with_logits(logits, mask)\n            dice_loss = dice_loss_fn(prob, mask) if dice_loss_fn else 0\n            bnd_loss = bnd_loss_fn(prob, dist) if bnd_loss_fn else 0\n            focal_loss = focal_loss_fn(logits, mask) if focal_loss_fn else 0\n            tversky_loss = tversky_loss_fn(logits, mask) if tversky_loss_fn else 0\n            adaptive_loss = adaptive_dice_fn(prob, mask) if adaptive_dice_fn else 0\n\n            loss = (w_ce * ce_loss + w_dice * dice_loss + w_bnd * bnd_loss +\n                    w_focal * focal_loss + w_tversky * tversky_loss +\n                    w_adaptive * adaptive_loss + w_bnd * bnd_loss)\n\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n            tr_ce += ce_loss.item()\n            tr_d += dice_loss.item() if dice_loss_fn else 0\n            tr_b += bnd_loss.item() if bnd_loss_fn else 0\n            tr_f += focal_loss.item() if focal_loss_fn else 0\n            tr_t += tversky_loss.item() if tversky_loss_fn else 0\n            tr_a += adaptive_loss.item() if adaptive_dice_fn else 0\n            tot += loss.item()\n\n        train_loss = tot / len(tr_dl)\n\n        model.eval()\n        val_ce = val_d = val_b = val_f = val_t = val_a = tval = val_acc = 0.0\n        val_bg_acc = val_fg_acc = 0.0\n\n        with torch.no_grad(), open(m_csv, 'a', newline='') as mf:\n            mw = csv.writer(mf)\n            for img, mask, dist, img_id in tqdm(vl_dl, desc=f\"[{branch_name}] Val\"):\n                img, mask, dist = img.to(device), mask.to(device), dist.to(device)\n                logits = model(img)\n                prob = torch.sigmoid(logits)\n\n                ce_loss = F.binary_cross_entropy_with_logits(logits, mask)\n                dice_loss = dice_loss_fn(prob, mask) if dice_loss_fn else 0\n                bnd_loss = bnd_loss_fn(prob, dist) if bnd_loss_fn else 0\n                focal_loss = focal_loss_fn(logits, mask) if focal_loss_fn else 0\n                tversky_loss = tversky_loss_fn(logits, mask) if tversky_loss_fn else 0\n                adaptive_loss = adaptive_dice_fn(prob, mask) if adaptive_dice_fn else 0\n\n                loss = (w_ce * ce_loss + w_dice * dice_loss + w_bnd * bnd_loss +\n                        w_focal * focal_loss + w_tversky * tversky_loss +\n                        w_adaptive * adaptive_loss + w_bnd * bnd_loss)\n\n                val_ce += ce_loss.item()\n                val_d += dice_loss.item() if dice_loss_fn else 0\n                val_b += bnd_loss.item() if bnd_loss_fn else 0\n                val_f += focal_loss.item() if focal_loss_fn else 0\n                val_t += tversky_loss.item() if tversky_loss_fn else 0\n                val_a += adaptive_loss.item() if adaptive_dice_fn else 0\n                tval += loss.item()\n\n                pm = (prob.cpu().numpy() > 0.5).astype(np.uint8)[0, 0]\n                gm = mask.cpu().numpy()[0, 0].astype(np.uint8)\n                dice, iou, bf1, biou, acc, prec, rec, f1, bg_acc, fg_acc = compute_metrics_per_image(pm, gm)\n                val_acc += acc\n                val_bg_acc += bg_acc\n                val_fg_acc += fg_acc\n\n                mw.writerow([img_id, epoch, branch_name, dice, iou, bf1, biou,\n                             acc, prec, rec, f1, bg_acc, fg_acc])\n\n        val_loss = tval / len(vl_dl)\n        avg_acc = val_acc / len(vl_dl)\n        avg_bg_acc = val_bg_acc / len(vl_dl)\n        avg_fg_acc = val_fg_acc / len(vl_dl)\n        t_elapsed = time.time() - t0\n\n        with open(l_csv, 'a', newline='') as lf:\n            csv.writer(lf).writerow([\n                epoch, train_loss, val_loss,\n                tr_ce / len(tr_dl), tr_d / len(tr_dl), tr_b / len(tr_dl), avg_acc, avg_bg_acc, avg_fg_acc,\n                t_elapsed\n            ])\n\n        print(f\"{branch_name} epoch{epoch}: tr_loss {train_loss:.4f} val_loss {val_loss:.4f}, \"\n              f\"avg_acc {avg_acc:.4f}, bg_acc {avg_bg_acc:.4f}, fg_acc {avg_fg_acc:.4f}, time {t_elapsed:.1f}s\")\n\n        torch.save(model.state_dict(), os.path.join(out_dir, f\"{branch_name}_{epoch}.pth\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"branches = {\n    'branch1_ce_only': {'w_ce': 1.0},\n    'branch2_dice_only': {'use_dice': True, 'w_dice': 1.0},\n    'branch3_ce_dice': {'use_dice': True, 'w_ce': 1.0, 'w_dice': 1.0},\n    'branch4_ce_focal': {'use_focal': True, 'w_ce': 1.0, 'w_focal': 1.0},\n    'branch5_ce_tversky': {'use_tversky': True, 'w_ce': 1.0, 'w_tversky': 1.0},\n    'branch6_ce_adaptive_dice': {'use_tvmf': True, 'w_ce': 1.0, 'w_adaptive': 1.0},\n    'branch7_ce_dice_focal': {'use_dice': True, 'use_focal': True, 'w_ce': 1.0, 'w_dice': 1.0, 'w_focal': 1.0},\n    'branch8_ce_dice_tversky': {'use_dice': True, 'use_tversky': True, 'w_ce': 1.0, 'w_dice': 1.0, 'w_tversky': 1.0},\n    'branch9_ce_adaptive_dice_focal': {'use_tvmf': True, 'use_focal': True, 'w_ce': 1.0, 'w_adaptive': 1.0, 'w_focal': 1.0},\n    'branch10_ce_adaptive_dice_tversky': {'use_tvmf': True, 'use_tversky': True, 'w_ce': 1.0, 'w_adaptive': 1.0, 'w_tversky': 1.0},\n    'branch11_ce_adaptive_dice_boundary': {'use_tvmf':True, 'use_boundary':True, 'w_ce': 1.0, 'w_dice':1.0, 'w_bnd':1.0},\n}\n\nfor name, cfg in branches.items():\n    torch.cuda.empty_cache()\n    gc.collect()\n    train_branch(name, cfg, train_df, val_df,\n                 out_dir=f\"/kaggle/working/experiments/{name}\",\n                 data_root=f\"/kaggle/input/kvasirseg/Kvasir-SEG/Kvasir-SEG\",\n                 BATCH=10, EPOCHS=30, LR=3e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Model","metadata":{}},{"cell_type":"code","source":"def test_model(val_df, model_path, root_path=\"/kaggle/input/kvasirseg/Kvasir-SEG/Kvasir-SEG\", seed=42, device=\"cuda\"):\n    # Load model\n    model = AttentionUNet(in_channels=3, num_classes=1).to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n\n    val_dataset = KvasirDataset(root_path=root_path, filename_df=val_df)\n    vl_dl = DataLoader(val_dataset, batch_size=2, shuffle=False,\n                       generator=torch.Generator().manual_seed(seed))\n\n    # Get 2 samples\n    data_iter = iter(vl_dl)\n    imgs, masks, _, filenames = next(data_iter)\n\n    imgs = imgs.to(device)\n    masks = masks.cpu().numpy()\n\n    with torch.no_grad():\n        outputs = model(imgs)\n        probs = torch.sigmoid(outputs)\n        preds = (probs > 0.5).float().cpu().numpy()\n\n    # Create a single figure for both images\n    fig, axes = plt.subplots(2, 3, figsize=(12, 8))  # 2 rows, 3 columns\n    \n    for i in range(2):\n        img_np = imgs[i].cpu().permute(1, 2, 0).numpy()\n        img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # normalize\n    \n        gt_mask = masks[i][0]\n        pred_mask = preds[i][0]\n    \n        axes[i, 0].imshow(img_np)\n        axes[i, 0].set_title(f\"Original Image\\n{filenames[i][0]}\")\n        axes[i, 0].axis('off')\n    \n        axes[i, 1].imshow(gt_mask, cmap='gray')\n        axes[i, 1].set_title(\"Ground Truth Mask\")\n        axes[i, 1].axis('off')\n    \n        axes[i, 2].imshow(pred_mask, cmap='gray')\n        axes[i, 2].set_title(\"Predicted Mask\")\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(model_path.split(\"/\")[-1].split(\".\")[0] + \".png\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(val_df, model_path=\"/kaggle/input/aunet-final/pytorch/default/1/branch7_ce_dice_focal_25.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}